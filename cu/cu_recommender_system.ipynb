{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CU Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are creating a cu recommender system by extractung feature vectors using PaddlePaddle, importing bets vectors into Milvus, and then searching in Milvus and Redis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "In this project, we use nsl-finance tenant data for 2021. This dataset contains approximately 3800 cus made under 160 Books and 2500 solutions. \n",
    "\n",
    "We use the following files:\n",
    "- movies.dat: Contains movie information.\n",
    "- movie_vectors.txt: Contains movie vectors that can be imported to Milvus easily.\n",
    "\n",
    "File structure:\n",
    "\n",
    " - movMovieID::Title::Genres   \n",
    "\n",
    "     - Titles are identical to titles provided by the IMDB (includingyear of release)\n",
    " \n",
    "     - Genres are pipe-separated\n",
    "\n",
    "     - Some MovieIDs do not correspond to a movie due to accidental duplicate entries and/or test entries\n",
    " \n",
    "    - Movies are mostly entered by hand, so errors and inconsistencies may exist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to package constraints, this notebook needs to be run using Python 3.6/3.7 . It is recommended that you use a virtual enviroment like Conda, instructions for installing Conda can be found [here](https://conda.io/projects/conda/en/latest/user-guide/install/index.html). \n",
    "\n",
    "Currently, there is a dirty workaround that you can use for python 3.8. When installing the `requirements.txt`, pip will fail to install`sentencepiece`. If you rerun the notebook after the install fails and avoid redownloading the packages, the rest of the notebook should run without any hiccups.\n",
    "\n",
    "|  Packages |  Servers |\n",
    "| --------------- | -------------- |\n",
    "| pymilvus==2.0.0rc5 | milvus-2.0.0-rc5 |\n",
    "| pymongo           | mongodb          |\n",
    "| paddle_serving_app |\n",
    "| paddlepaddle==2.1.1 |\n",
    "\n",
    "\n",
    "We have included a requirements.txt file in order to easily satisfy the required packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Up and Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Packages\n",
    "Install the required python packages with `requirements.txt`. If using Python 3.8, look at workaround in the Requirements section. Uninstall previous pymilvus-orm if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Collecting pymilvus==2.0.0rc6\n",
      "  Using cached pymilvus-2.0.0rc6-py3-none-any.whl (117 kB)\n",
      "Requirement already satisfied: pymongo in /opt/homebrew/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (3.12.0)\n",
      "Collecting flask-cors\n",
      "  Using cached Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy in /Users/rverma/Library/Python/3.9/lib/python/site-packages (from -r requirements.txt (line 6)) (1.21.2)\n",
      "Collecting flask\n",
      "  Using cached Flask-2.0.2-py3-none-any.whl (95 kB)\n",
      "Collecting flask_restful\n",
      "  Using cached Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.1.0.tar.gz (78 kB)\n",
      "Collecting grpcio==1.37.1\n",
      "  Using cached grpcio-1.37.1.tar.gz (21.7 MB)\n",
      "Collecting ujson>=2.0.0\n",
      "  Using cached ujson-4.2.0.tar.gz (7.1 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting mmh3\n",
      "  Using cached mmh3-3.0.0-cp39-cp39-macosx_11_0_arm64.whl (13 kB)\n",
      "Collecting grpcio-tools==1.37.1\n",
      "  Using cached grpcio-tools-1.37.1.tar.gz (2.1 MB)\n",
      "Collecting pandas==1.2.4\n",
      "  Using cached pandas-1.2.4.tar.gz (5.5 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting requests>=2.22.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/homebrew/lib/python3.9/site-packages (from grpcio==1.37.1->pymilvus==2.0.0rc6->-r requirements.txt (line 1)) (1.16.0)\n",
      "Collecting protobuf<4.0dev,>=3.5.0.post1\n",
      "  Downloading protobuf-3.18.1-py2.py3-none-any.whl (174 kB)\n",
      "\u001b[K     |████████████████████████████████| 174 kB 1.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/homebrew/lib/python3.9/site-packages (from grpcio-tools==1.37.1->pymilvus==2.0.0rc6->-r requirements.txt (line 1)) (57.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/homebrew/lib/python3.9/site-packages (from pandas==1.2.4->pymilvus==2.0.0rc6->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/rverma/Library/Python/3.9/lib/python/site-packages (from pandas==1.2.4->pymilvus==2.0.0rc6->-r requirements.txt (line 1)) (2021.3)\n",
      "Collecting Werkzeug>=2.0\n",
      "  Downloading Werkzeug-2.0.2-py3-none-any.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 10.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=3.0 in /opt/homebrew/lib/python3.9/site-packages (from flask->-r requirements.txt (line 7)) (3.0.2)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: click>=7.1.2 in /Users/rverma/Library/Python/3.9/lib/python/site-packages (from flask->-r requirements.txt (line 7)) (8.0.1)\n",
      "Collecting aniso8601>=0.82\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 8.0 MB/s \n",
      "\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 34.3 MB/s \n",
      "\u001b[?25hCollecting tokenizers>=0.10.3\n",
      "  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 17.1 MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[K     |████████████████████████████████| 76 kB 26.0 MB/s \n",
      "\u001b[?25hCollecting torch>=1.6.0\n",
      "  Downloading torch-1.9.0-cp39-none-macosx_11_0_arm64.whl (41.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 41.5 MB 23.1 MB/s \n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.10.0-cp39-cp39-macosx_11_0_arm64.whl (499 kB)\n",
      "\u001b[K     |████████████████████████████████| 499 kB 119.9 MB/s \n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit-learn-1.0.tar.gz (7.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.8 MB 12.9 MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-^C\n",
      "\u001b[?25canceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting Milvus Server\n",
    "\n",
    "This demo uses Milvus 2.0 Standalone with docker-compose, please refer to [Install Milvus 2.0](https://milvus.io/docs/v2.0.0/install_standalone-docker.md) for other installation options (on Kubernetes or use Milvus Cluster). Currently we have deployed standalone milvus in dev kubernetes. We need to grep the endpoint by pod ip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting Redis Server\n",
    "We are using Redis as a metadata storage service. Code can easily be modified to use a python dictionary, but that usually does not work in any use case outside of quick examples. We need a metadata storage service in order to be able to be able to map between embeddings and the corresponding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run  --name redis -d -p 6379:6379 redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm Running Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def _connect_mongo(host, port, username, password, db):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        mongo_uri = 'mongodb://%s:%s@%s:%s/%s?authSource=%s&readPreference=primary&ssl=false' % (username, urllib.parse.quote(password), host, port, db,'nsl_bet_db_qa3')\n",
    "        conn = MongoClient(mongo_uri)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "\n",
    "    return conn[db]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mongo(db, collection, query={}, host='10.220.98.254', port=27017, username='bet', password='bet@123', no_id=False):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "\n",
    "    # Connect to MongoDB\n",
    "    db = _connect_mongo(host=host, port=port, username=username, password=password, db=db)\n",
    "\n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].aggregate(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "    \n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsi_pipeline = [\n",
    "    {\n",
    "        \"$project\": {\"nb\": \"$$ROOT\", \"_id\": 0}\n",
    "    },\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"localField\": \"nb.gsiList.id\",\n",
    "            \"from\": \"nsl_gsi\",\n",
    "            \"foreignField\": \"id\",\n",
    "            \"as\": \"ng\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$unwind\": {\n",
    "            \"path\": \"$ng\",\n",
    "            \"preserveNullAndEmptyArrays\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$lookup\": {\n",
    "            \"localField\": \"ng.solutionLogic.referencedChangeUnit\",\n",
    "            \"from\": \"nsl_change_unit\",\n",
    "            \"foreignField\": \"id\",\n",
    "            \"as\": \"cu\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$unwind\": {\n",
    "            \"path\": \"$cu\",\n",
    "            \"preserveNullAndEmptyArrays\": False\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$addFields\": {\n",
    "            \"bookId\": \"$nb.displayName\",\n",
    "            \"gsiId\": \"$ng.displayName\",\n",
    "            \"bookName\": \"$nb.displayName\",\n",
    "            \"gsiName\": \"$ng.displayName\",\n",
    "            \"cuName\": \"$cu.displayName\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$match\": {\n",
    "            \"$and\": [\n",
    "                {\n",
    "                    \"nb.tenantId\": {\n",
    "                        \"$in\": [\n",
    "                            \"ProjectCarnivals\",\n",
    "                            \"Banking\",\n",
    "                            \"projectmanagement\",\n",
    "                            \"Brane-Finance\",\n",
    "                            \"FinanceSolBrane\",\n",
    "                            \"AILABS\",\n",
    "                            \"BRF2008\",\n",
    "                            \"Finance2008\",\n",
    "                            \"FamilyApp2008\",\n",
    "                            \"Learning2008\",\n",
    "                            \"GRC\",\n",
    "                            \"Insurance\",\n",
    "                            \"Healthcare\",\n",
    "                            \"SupplyChain\",\n",
    "                            \"Pharma\",\n",
    "                            \"CustomerSuccess\"]\n",
    "                    }\n",
    "                },\n",
    "                {\"nb.displayName\": {\"$regex\": \"^((?!test).)*$\", \"$options\": \"i\"}},\n",
    "                {\"nb.displayName\": {\"$regex\": \"^((?!book).)*$\", \"$options\": \"i\"}},\n",
    "                {\"ng.displayName\": {\"$regex\": \"^((?!test).)*$\", \"$options\": \"i\"}}\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": {\"bookId\": \"$bookId\", \"gsiId\": \"$gsiId\"},\n",
    "            \"addToSet(cu_name)\": {\"$addToSet\": \"$cu.displayName\"}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"bookId\": \"$_id.bookId\",\n",
    "            \"gsiId\": \"$_id.gsiId\",\n",
    "            \"cuId\": \"$addToSet(cu_name)\",\n",
    "            \"bookName\": \"$nb.displayName\",\n",
    "            \"gsiName\": \"$ng.displayName\",\n",
    "            \"cuName\": \"$cu.displayName\",\n",
    "            \"_id\": 0\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_mongo(db='nsl_bet_db_soln',collection='nsl_book',query=gsi_pipeline)\n",
    "df.to_pickle(\"data/cu.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bookId</th>\n",
       "      <th>gsiId</th>\n",
       "      <th>cuId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Covid Relief App</td>\n",
       "      <td>Emergency Helplines for Covid</td>\n",
       "      <td>[View Helplines for Covid, CR_Helpline_Search]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kids</td>\n",
       "      <td>New Solution Building</td>\n",
       "      <td>[Quiz Category, Login ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cus_999</td>\n",
       "      <td>Policy Renewal</td>\n",
       "      <td>[Display Renewal Amount, Grace Period Renewal,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>newcarriermaster1</td>\n",
       "      <td>New Carrier Master</td>\n",
       "      <td>[Send Email Reserved CU, search reserved cu, R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Governance Risk and Compliance</td>\n",
       "      <td>SearchAuditWebex</td>\n",
       "      <td>[SearchWebex, Displaycu, Auditee head option, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           bookId                          gsiId                                               cuId\n",
       "0                Covid Relief App  Emergency Helplines for Covid     [View Helplines for Covid, CR_Helpline_Search]\n",
       "1                            Kids          New Solution Building                            [Quiz Category, Login ]\n",
       "2                         cus_999                 Policy Renewal  [Display Renewal Amount, Grace Period Renewal,...\n",
       "3               newcarriermaster1             New Carrier Master  [Send Email Reserved CU, search reserved cu, R...\n",
       "4  Governance Risk and Compliance               SearchAuditWebex  [SearchWebex, Displaycu, Auditee head option, ..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_pickle(\"data/cu.pickle\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Movies into Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Connectings to Milvus and Redis\n",
    "Both servers are running as Docker containers on the localhost with their corresponding default ports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import *\n",
    "import redis\n",
    "\n",
    "# connections.connect()\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "r = redis.StrictRedis(host=\"localhost\", port=6379) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading Movies into Redis\n",
    "We begin by loading all the movie files into redis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "\n",
    "#1::Toy Story (1995)::Animation|Children's|Comedy\n",
    "def process_movie(lines, redis_cli):\n",
    "    for line in lines:\n",
    "        if len(line.strip()) == 0:\n",
    "            continue\n",
    "        tmp = line.strip().split(\"::\")\n",
    "        movie_id = tmp[0]\n",
    "        title = tmp[1]\n",
    "        genre_group = tmp[2]\n",
    "        tmp = genre_group.strip().split(\"|\")\n",
    "        genre = tmp\n",
    "        movie_info = {\"movie_id\" : movie_id,\n",
    "                \"title\" : title,\n",
    "                \"genre\" : genre\n",
    "                }\n",
    "        redis_cli.set(\"{}##movie_info\".format(movie_id), json.dumps(movie_info))\n",
    "        \n",
    "with codecs.open(\"movie_recommender/movies.dat\", \"r\",encoding='utf-8',errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "        process_movie(lines, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creating Partition and Collection in Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_NAME = 'demo_films'\n",
    "PARTITION_NAME = 'Movie'\n",
    "\n",
    "pk = FieldSchema(name='pk', dtype=DataType.INT64, is_primary=True, auto_id=False)\n",
    "field = FieldSchema(name='vec', dtype=DataType.FLOAT_VECTOR, dim=32)\n",
    "schema = CollectionSchema(fields=[pk, field], description=\"movie recommender: demo films\")\n",
    "\n",
    "if utility.get_connection().has_collection(COLLECTION_NAME): # drop the same collection created before\n",
    "    collection = Collection(COLLECTION_NAME)\n",
    "    collection.drop()\n",
    "else:\n",
    "    collection = Collection(name=COLLECTION_NAME, schema=schema)\n",
    "    partition = collection.create_partition(PARTITION_NAME)\n",
    "    print(\"Collection & partition are successfully created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Getting Embeddings and IDs\n",
    "The vectors in `movie_vectors.txt` are obtained from the `user_vector_model` downloaded above. So we can directly get the vectors and the IDs by reading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors():\n",
    "    with codecs.open(\"movie_recommender/movie_vectors.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "        lines = f.readlines()\n",
    "    ids = [int(line.split(\":\")[0]) for line in lines]\n",
    "    embeddings = []\n",
    "    for line in lines:\n",
    "        line = line.strip().split(\":\")[1][1:-1]\n",
    "        str_nums = line.split(\",\")\n",
    "        emb = [float(x) for x in str_nums]\n",
    "        embeddings.append(emb)\n",
    "    return ids, embeddings\n",
    "\n",
    "ids, embeddings = get_vectors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Importing Vectors into Milvus\n",
    "Import vectors into the partition **Movie** under the collection **demo_films**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if collection.num_entities != 0:\n",
    "    print(COLLECTION_NAME + \" is not empty!\")  \n",
    "else:\n",
    "    mr = collection.insert(data=[ids,embeddings], partition_name=PARTITION_NAME)\n",
    "\n",
    "print(\"Record count in collection: \" + str(collection.num_entities))\n",
    "# print(str(len(mr.primary_keys)) + \" ids:\\n\", mr.primary_keys[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flush collection with inserted vectors to disk\n",
    "# utility.get_connection().flush([COLLECTION_NAME])\n",
    "\n",
    "index_param = {\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"index_type\":\"IVF_FLAT\",\n",
    "    \"params\":{\"nlist\":128}\n",
    "}\n",
    "\n",
    "collection.create_index(field_name=\"vec\", index_params=index_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recalling Vectors in Milvus\n",
    "#### 1. Genarating User Embeddings\n",
    "Pass in the gender, age and occupation of the user we want to recommend. **user_vector_model** model will generate the corresponding user vector.\n",
    "Occupation is chosen from the following choices:\n",
    "*  0:  \"other\" or not specified\n",
    "*  1:  \"academic/educator\"\n",
    "*  2:  \"artist\"\n",
    "*  3:  \"clerical/admin\"\n",
    "*  4:  \"college/grad student\"\n",
    "*  5:  \"customer service\"\n",
    "*  6:  \"doctor/health care\"\n",
    "*  7:  \"executive/managerial\"\n",
    "*  8:  \"farmer\"\n",
    "*  9:  \"homemaker\"\n",
    "*  10:  \"K-12 student\"\n",
    "*  11:  \"lawyer\"\n",
    "*  12:  \"programmer\"\n",
    "*  13:  \"retired\"\n",
    "*  14:  \"sales/marketing\"\n",
    "*  15:  \"scientist\"\n",
    "*  16:  \"self-employed\"\n",
    "*  17:  \"technician/engineer\"\n",
    "*  18:  \"tradesman/craftsman\"\n",
    "*  19:  \"unemployed\"\n",
    "*  20:  \"writer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from paddle_serving_app.local_predict import LocalPredictor\n",
    "\n",
    "class RecallServerServicer(object):\n",
    "    def __init__(self):\n",
    "        self.uv_client = LocalPredictor()\n",
    "        self.uv_client.load_model_config(\"movie_recommender/user_vector_model/serving_server_dir\") \n",
    "        \n",
    "    def hash2(self, a):\n",
    "        return hash(a) % 1000000\n",
    "\n",
    "    def get_user_vector(self):\n",
    "        dic = {\"userid\": [], \"gender\": [], \"age\": [], \"occupation\": []}\n",
    "        lod = [0]\n",
    "        dic[\"userid\"].append(self.hash2('0'))\n",
    "        dic[\"gender\"].append(self.hash2('M'))\n",
    "        dic[\"age\"].append(self.hash2('23'))\n",
    "        dic[\"occupation\"].append(self.hash2('6'))\n",
    "        lod.append(1)\n",
    "\n",
    "        dic[\"userid.lod\"] = lod\n",
    "        dic[\"gender.lod\"] = lod\n",
    "        dic[\"age.lod\"] = lod\n",
    "        dic[\"occupation.lod\"] = lod\n",
    "        for key in dic:\n",
    "            dic[key] = np.array(dic[key]).astype(np.int64).reshape(len(dic[key]),1)\n",
    "        fetch_map = self.uv_client.predict(feed=dic, fetch=[\"save_infer_model/scale_0.tmp_1\"], batch=True)\n",
    "        return fetch_map[\"save_infer_model/scale_0.tmp_1\"].tolist()[0]\n",
    "\n",
    "recall = RecallServerServicer()\n",
    "user_vector = recall.get_user_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Searching\n",
    "Pass in the user vector, and then recall vectors in the previously imported data collection and partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.load() # load collection memory before search\n",
    "\n",
    "topK = 20\n",
    "SEARCH_PARAM = {\n",
    "    \"metric_type\":\"L2\",\n",
    "    \"params\":{\"nprobe\": 20},\n",
    "    }\n",
    "results = collection.search([user_vector],\"vec\",param=SEARCH_PARAM, limit=topK, expr=None, output_fields=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Returning Information by IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = []\n",
    "for x in results:\n",
    "    for y in x.ids:\n",
    "        I.append(y)\n",
    "        \n",
    "recall_results = []\n",
    "for x in I:\n",
    "    recall_results.append(r.get(\"{}##movie_info\".format(x)).decode('utf-8'))\n",
    "recall_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the recall service, the results can be further sorted using the **movie_recommender** model, and then the movies with high similarity scores can be recommended to users. You can try this deployable recommender system using this [quick start](QUICK_START.md)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
